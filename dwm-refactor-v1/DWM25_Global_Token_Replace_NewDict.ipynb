{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "from textdistance import Levenshtein\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalReplace(logFile, sampleName, minFreqStdToken, minLenStdToken, maxFreqErrToken):\n",
    "    print (\"\\n>>Starting DWM25 --- runReplacement is set to True, starting global token replacement\")\n",
    "    print(\"\\n>>Starting DWM25 --- runReplacement is set to True, starting global token replacement\", file=logFile)\n",
    "    Class = Levenshtein()\n",
    "    NewDict = json.load(open(\"NewDict.txt\"))\n",
    "    print(\"Dictionary Load =\", len(NewDict))\n",
    "    print(type(NewDict))\n",
    "#Phase 1: Build token dictionary from tokenized file\n",
    "    tokenizedFileName = sampleName+'-Tokenized.txt'\n",
    "    tokenizedFile = open(tokenizedFileName,'r')\n",
    "    print('Global Replace Cycle')\n",
    "    print('Global Replace Cycle', file=logFile)\n",
    "    index ={}\n",
    "    refCnt = 0\n",
    "    tokenCnt = 0\n",
    "    line = tokenizedFile.readline()\n",
    "    while line !='':\n",
    "        refCnt +=1\n",
    "        line = line.strip()\n",
    "        tokenList = re.split('\\s+', line)\n",
    "        refID = tokenList[0]\n",
    "        for j in range(1,len(tokenList)):\n",
    "            token= tokenList[j]\n",
    "            tokenCnt +=1\n",
    "            if token!='':\n",
    "                if token not in index:\n",
    "                    index[token]=1\n",
    "                else:\n",
    "                    count = index.get(token)\n",
    "                    count = count + 1\n",
    "                    index[token]=count      \n",
    "        line = tokenizedFile.readline()\n",
    "    tokenizedFile.close()\n",
    "    print('Total References=',refCnt)\n",
    "    print('Total Tokens=',tokenCnt)\n",
    "    print('Unique Tokens=',len(index))\n",
    "    print(\"Minimum Frequency of Standard Token = \", minFreqStdToken)\n",
    "    print(\"Minimum Length of Standard Token = \", minLenStdToken)\n",
    "    print(\"Maximum Frequency of Error Token = \", maxFreqErrToken)\n",
    "    print('Total References=',refCnt, file=logFile)\n",
    "    print('Total Tokens=',tokenCnt, file=logFile)\n",
    "    print('Unique Tokens=',len(index), file=logFile)\n",
    "    print(\"Minimum Frequency of Standard Token = \", minFreqStdToken, file=logFile)\n",
    "    print(\"Minimum Length of Standard Token = \", minLenStdToken, file=logFile)\n",
    "    print(\"Maximum Frequency of Error Token = \", maxFreqErrToken, file=logFile)\n",
    "#Phase 2, build list of token-frequency pairs and sort descending by token frequency\n",
    "    print('References Processed=',refCnt)\n",
    "    print('References Processed=',refCnt, file=logFile)\n",
    "    sortedIndex = sorted(index.items(),reverse=True, key=operator.itemgetter(1))\n",
    "    tokenCnt = len(sortedIndex)\n",
    "    print(\"Sorted Token Size =\", tokenCnt)\n",
    "    print(\"Sorted Token Size =\", tokenCnt, file=logFile)\n",
    "    cleanIndex = []\n",
    "    for j in range(0,tokenCnt):\n",
    "        pairJ = sortedIndex[j]\n",
    "        wordJ = pairJ[0]\n",
    "        lenJ = len(wordJ)\n",
    "        freqJ = pairJ[1]\n",
    "        if lenJ<minLenStdToken:\n",
    "            continue\n",
    "        if not wordJ.isalpha():\n",
    "            continue\n",
    "        cleanIndex.append(pairJ)\n",
    "    cleanCnt = len(cleanIndex)\n",
    "    print(\"Clean Token Size =\", cleanCnt)\n",
    "    print(\"Clean Token Size =\", cleanCnt, file=logFile)\n",
    "#Phase 3 Build Dictionary (stdToken) of token corrections\n",
    "    stdToken = {}\n",
    "    checkCnt = 0\n",
    "    changeTable = open('Token_Substitution_Table.txt','w')\n",
    "    for j in range(0,cleanCnt-1):\n",
    "        pairJ = cleanIndex[j]\n",
    "        wordJ = pairJ[0]\n",
    "        lenJ = len(wordJ)\n",
    "        freqJ = pairJ[1]\n",
    "        if freqJ < minFreqStdToken:\n",
    "            print(\"*Stop Replacements here\")\n",
    "            print(\"*Stop Replacements here\", file=logFile)\n",
    "            break\n",
    "        for k in range(cleanCnt-1, 1, -1):\n",
    "            pairK = cleanIndex[k]\n",
    "            wordK = pairK[0]\n",
    "            lenK = len(wordK)\n",
    "            freqK = pairK[1]\n",
    "            if k == j+1:\n",
    "                break\n",
    "            if Class.distance(wordJ,wordK)==1 and freqK<=maxFreqErrToken and not (wordK.lower() in NewDict):\n",
    "                stdToken[wordK] = wordJ\n",
    "                string = wordJ+'\\t'+str(freqJ)+'\\t'+wordK+'\\t'+str(freqK)\n",
    "                changeTable.write(string+'\\n')\n",
    "                cleanIndex[k] = ('',freqK)\n",
    "                checkCnt = checkCnt + freqK\n",
    "    replacementPairs = len(stdToken)\n",
    "    changeTable.close\n",
    "#Phase 4, re-read washed file and replace tokens in the stdToken dictionary, write to CleanedFile\n",
    "    tokenizedFile = open(tokenizedFileName, 'r')\n",
    "    tokenReplaceFileName = sampleName+'-TokenReplace.txt'\n",
    "    tokenReplaceFile = open(tokenReplaceFileName,'w')\n",
    "    changeCnt = 0\n",
    "    tokenCnt = 0\n",
    "    line = tokenizedFile.readline()\n",
    "    refCnt = 0\n",
    "    refsChanged = 0\n",
    "    while line !='':\n",
    "        refCnt +=1\n",
    "        line = line.strip()\n",
    "        tokenList = re.split('\\s+', line)\n",
    "        refID = tokenList[0]\n",
    "        outLine = refID\n",
    "        change = False\n",
    "        for j in range(1,len(tokenList)):\n",
    "            token= tokenList[j]\n",
    "            tokenCnt +=1\n",
    "            if token in stdToken:\n",
    "                change = True\n",
    "                oldToken = token\n",
    "                token = stdToken[oldToken]\n",
    "                changeCnt +=1\n",
    "                #print(refID, oldToken,token)\n",
    "            outLine = outLine + ' '+token\n",
    "        if change:\n",
    "            refsChanged +=1\n",
    "        outLine = outLine + '\\n'\n",
    "        tokenReplaceFile.write(outLine)           \n",
    "        line = tokenizedFile.readline()\n",
    "    tokenizedFile.close()\n",
    "    tokenReplaceFile.close()\n",
    "    print(\"References Processed = \", refCnt)\n",
    "    print(\"Total Replacement Pairs =\",replacementPairs)\n",
    "    print(\"Tokens Read =\",tokenCnt)\n",
    "    print(\"Tokens Changed = \",changeCnt)\n",
    "    print(\"References Changed =\",refsChanged)\n",
    "    print(\"References Processed = \", refCnt, file=logFile)\n",
    "    print(\"Total Replacement Pairs =\",replacementPairs, file=logFile)\n",
    "    print(\"Tokens Read =\",tokenCnt, file=logFile)\n",
    "    print(\"Tokens Changed = \",changeCnt, file=logFile)\n",
    "    print(\"References Changed =\",refsChanged, file=logFile)\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
