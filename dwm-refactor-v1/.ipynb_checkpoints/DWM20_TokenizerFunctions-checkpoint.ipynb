{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "def tokenizeInput(parms, tokenFreqDict, tokenizedFileName):\n",
    "    #***********Inner Function*******************************\n",
    "    #Replace delimiter with blanks, then compress token by replacing non-word characters with null\n",
    "    def tokenizerCompress(string):\n",
    "        string = string.upper()\n",
    "        string = string.replace(delimiter,' ')\n",
    "        tokenList = re.split('[\\s]+',string)\n",
    "        newList = []\n",
    "        for token in tokenList:\n",
    "            newToken = re.sub('[\\W]+','',token)\n",
    "            if len(newToken)>0:\n",
    "                newList.append(newToken)\n",
    "        return newList\n",
    "    #***********Inner Function*******************************\n",
    "    #Replace all non-words characters with blanks, then split on blanks\n",
    "    def tokenizerSplitter(string):\n",
    "        string = string.upper()\n",
    "        string = re.sub('[\\W]+',' ',string)\n",
    "        tokenList = re.split('[\\s]+',string)\n",
    "        newList = []\n",
    "        for token in tokenList:\n",
    "            if len(token)>0:\n",
    "                newList.append(token)\n",
    "        return newList\n",
    "    #***********Outer Main Function*******************************\n",
    "    # Start of Main Tokenizer Function\n",
    "    inputFileName = parms['inputFileName']\n",
    "    inputFile= open(inputFileName,'r')\n",
    "    tokenizerType = parms['tokenizerType']\n",
    "    goodType = False\n",
    "    if tokenizerType=='Splitter':\n",
    "        tokenizerFunction = tokenizerSplitter\n",
    "        goodType = True\n",
    "    if tokenizerType=='Compress':\n",
    "        tokenizerFunction = tokenizerCompress\n",
    "        goodType = True\n",
    "    if goodType == False:\n",
    "        print('**Error: Invalid Parameter value for tokenizerType ',tokenizerType)\n",
    "        sys.exit()\n",
    "    #Phase 1, read input file, tokenize, wash tokens, and write to Sample-Tokenized.txt file\n",
    "    washedFile = open(tokenizedFileName,'w')\n",
    "    print('Tokenizing References')\n",
    "    refCnt = 0\n",
    "    # skip header record\n",
    "    hasHeader = parms['hasHeader']\n",
    "    if hasHeader:\n",
    "        line = inputFile.readline()\n",
    "    line = inputFile.readline()\n",
    "    tokenCnt = 0\n",
    "    tokensOut = 0\n",
    "    delimiter = parms['delimiter']\n",
    "    removeDuplicateTokens = parms['removeDuplicateTokens']\n",
    "    while line !='':\n",
    "        refCnt +=1\n",
    "        line = line.strip()\n",
    "        firstDelimiter = line.find(delimiter)\n",
    "        refID = line[0:firstDelimiter]\n",
    "        body = line[firstDelimiter+1:]\n",
    "        tokenList = tokenizerFunction(body)\n",
    "        tokenCnt = tokenCnt + len(tokenList)\n",
    "        if removeDuplicateTokens:\n",
    "            tokenList = list(dict.fromkeys(tokenList))\n",
    "        tokensOut = tokensOut + len(tokenList)\n",
    "        outLine = ''\n",
    "        for j in range(0,len(tokenList)):\n",
    "            token= tokenList[j]\n",
    "            outLine = outLine+' '+token\n",
    "            if token in tokenFreqDict:\n",
    "                tokenFreqDict[token] +=1\n",
    "            else:\n",
    "                tokenFreqDict[token] = 1\n",
    "        outLine = refID+outLine + '\\n'\n",
    "        washedFile.write(outLine)           \n",
    "        line = inputFile.readline()\n",
    "    inputFile.close()\n",
    "    washedFile.close()\n",
    "    print('Total References=',refCnt)\n",
    "    print('Total Tokens Found =',tokenCnt)\n",
    "    print('Total Tokens Output =', tokensOut)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
